{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Question 1: What is Ensemble Learning in machine learning? Explain the key idea behind it."
      ],
      "metadata": {
        "id": "k1bRaqT_y15T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Answer: Ensemble Learning is a technique in machine learning where multiple models (often called \"weak learners\") are combined to create a stronger, more accurate model (the \"ensemble\").\n",
        "Key Idea Behind Ensemble Learning\n",
        "- Diversity and Combination: Ensemble methods leverage the diversity of multiple models to improve overall performance. By combining predictions (through voting, averaging, etc.), the ensemble reduces the impact of individual model errors.\n",
        "- Improving Accuracy and Robustness: Ensembles can improve accuracy over single models and make the overall model more robust to noise and overfitting.\n"
      ],
      "metadata": {
        "id": "wu2eX76xy4dq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 2: What is the difference between Bagging and Boosting?"
      ],
      "metadata": {
        "id": "W9P8WlC6zA9b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Answer: Bagging and Boosting are two popular ensemble learning techniques used in machine learning. Here's how they differ:\n",
        "- Purpose and Approach:\n",
        "- Bagging (Bootstrap Aggregating): Reduces variance by training models on different bootstrap samples of the data and averaging predictions.\n",
        "- Boosting: Reduces bias by sequentially training models, with each new model focusing on the errors of the previous ones.\n",
        "- Model Training:\n",
        "- Bagging: Models are trained independently on different data subsets.\n",
        "- Boosting: Models are trained sequentially, with later models correcting earlier ones.\n",
        "- Effect on Performance:\n",
        "- Bagging: Helps reduce overfitting and variance.\n",
        "- Boosting: Can reduce bias and improve accuracy but might lead to overfitting if not tuned properly.\n",
        "\n",
        "Examples of Algorithms\n",
        "- Bagging: Random Forest (an ensemble of decision trees).\n",
        "- Boosting: AdaBoost, Gradient Boosting Machines (like XGBoost).\n"
      ],
      "metadata": {
        "id": "9kdoKFFYyul5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 3: What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?"
      ],
      "metadata": {
        "id": "QLCdkL1nzQ5d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: Bootstrap sampling is a statistical technique where you create multiple subsets of data by sampling with replacement from the original dataset. Each bootstrap sample is typically the same size as the original dataset but might contain duplicates of some data points and omit others.\n",
        "\n",
        "Role in Bagging Methods like Random Forest\n",
        "In Bagging (Bootstrap Aggregating) methods like Random Forest:\n",
        "- Creating Diverse Trees: Each decision tree in the Random Forest is trained on a different bootstrap sample of the data. This introduces diversity among the trees.\n",
        "- Reducing Overfitting: By averaging predictions from trees trained on different bootstrap samples, Bagging reduces variance and helps prevent overfitting.\n",
        "- Improving Stability: The overall model becomes more stable and often more accurate due to the aggregation of multiple trees.\n"
      ],
      "metadata": {
        "id": "3mKGV9tczVCt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?"
      ],
      "metadata": {
        "id": "4ovA-p12za9X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Answer: In Bagging methods like Random Forest, Out-of-Bag (OOB) samples refer to the data points that are not included in a particular bootstrap sample used to train a tree. On average, about 1/3 of the data points are left out of each bootstrap sample.\n",
        "\n",
        "OOB Score for Evaluating Ensemble Models\n",
        "The OOB score is an estimate of the model's performance:\n",
        "- How it's calculated: For each data point, use only the trees where that data point was OOB (not in the bootstrap sample used for training) to make predictions. Aggregate these predictions to get an OOB estimate for that data point.\n",
        "- Use for evaluation: The OOB score (like OOB accuracy for classification) gives an unbiased estimate of the model's generalization performance without needing a separate test set.\n"
      ],
      "metadata": {
        "id": "AVQqYr5jze5-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 5: Compare feature importance analysis in a single Decision Tree vs. a Random Forest."
      ],
      "metadata": {
        "id": "GRYqB79VzkdH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: Feature importance can be calculated differently in a single Decision Tree versus a Random Forest.\n",
        "\n",
        "- Single Decision Tree:\n",
        "- Feature importance is based on how much each feature contributes to reducing impurity (like Gini impurity for classification) in the tree.\n",
        "- Can be highly dependent on the specific tree structure and prone to overfitting.\n",
        "- Random Forest:\n",
        "- Feature importance is typically averaged across all trees in the forest.\n",
        "- Provides a more robust and stable estimate of feature importance due to averaging over many trees.\n"
      ],
      "metadata": {
        "id": "TvWd6tAgzp23"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 6: Write a Python program to:\n",
        "\n",
        "●\tLoad the Breast Cancer dataset using\n",
        "\n",
        "sklearn.datasets.load_breast_cancer()\n",
        "\n",
        "●\tTrain a Random Forest Classifier\n",
        "\n",
        "●\tPrint the top 5 most important features based on feature importance scores.\n",
        "\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n"
      ],
      "metadata": {
        "id": "jdctUxrUzwzQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "def main():\n",
        "    # Load the Breast Cancer dataset\n",
        "    data = load_breast_cancer()\n",
        "    X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "    y = data.target\n",
        "\n",
        "    # Train a Random Forest Classifier\n",
        "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    model.fit(X, y)\n",
        "\n",
        "    # Get feature importances\n",
        "    importances = model.feature_importances_\n",
        "    feature_names = data.feature_names\n",
        "\n",
        "    # Print top 5 most important features\n",
        "    top_features = sorted(zip(feature_names, importances), key=lambda x: x[1], reverse=True)[:5]\n",
        "    print(\"Top 5 most important features:\")\n",
        "    for feature, importance in top_features:\n",
        "        print(f\"{feature}: {importance:.3f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJYkgdQqz4t6",
        "outputId": "4cabc787-8113-433d-b1d4-9d237d89f29a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 most important features:\n",
            "worst area: 0.139\n",
            "worst concave points: 0.132\n",
            "mean concave points: 0.107\n",
            "worst radius: 0.083\n",
            "worst perimeter: 0.081\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 7: Write a Python program to:\n",
        "\n",
        "●\tTrain a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "\n",
        "●\tEvaluate its accuracy and compare with a single Decision Tree\n",
        "\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n"
      ],
      "metadata": {
        "id": "Ob6zSHXs0Hzj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "def main():\n",
        "    # Load the Iris dataset\n",
        "    iris = load_iris()\n",
        "    X = iris.data\n",
        "    y = iris.target\n",
        "    # Split data into train/test sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Train a single Decision Tree\n",
        "    single_dt = DecisionTreeClassifier(random_state=42)\n",
        "    single_dt.fit(X_train, y_train)\n",
        "    single_dt_pred = single_dt.predict(X_test)\n",
        "    single_dt_accuracy = accuracy_score(y_test, single_dt_pred)\n",
        "\n",
        "    # Train a Bagging Classifier using Decision Trees\n",
        "    bagging_clf = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\n",
        "    bagging_clf.fit(X_train, y_train)\n",
        "    bagging_pred = bagging_clf.predict(X_test)\n",
        "    bagging_accuracy = accuracy_score(y_test, bagging_pred)\n",
        "\n",
        "    # Compare accuracies\n",
        "    print(f\"**Single Decision Tree Accuracy**: {single_dt_accuracy:.3f}\")\n",
        "    print(f\"**Bagging Classifier Accuracy**: {bagging_accuracy:.3f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehwDxQtL0ODN",
        "outputId": "59a91d03-9f5a-4fbf-a882-38a8c2fcfc18"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Single Decision Tree Accuracy**: 1.000\n",
            "**Bagging Classifier Accuracy**: 1.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 8: Write a Python program to:\n",
        "\n",
        "●\tTrain a Random Forest Classifier\n",
        "\n",
        "●\tTune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "\n",
        "●\tPrint the best parameters and final accuracy\n",
        "\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n"
      ],
      "metadata": {
        "id": "RHvHDW5l0eB2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def main():\n",
        "    # Load the Breast Cancer dataset\n",
        "    data = load_breast_cancer()\n",
        "    X = data.data\n",
        "    y = data.target\n",
        "\n",
        "    # Split data into train/test sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define hyperparameter grid\n",
        "    param_grid = {\n",
        "        'max_depth': [3, 5, 10, None],\n",
        "        'n_estimators': [50, 100, 200]\n",
        "    }\n",
        "\n",
        "    # Train a Random Forest Classifier with GridSearchCV\n",
        "    model = RandomForestClassifier(random_state=42)\n",
        "    grid_search = GridSearchCV(model, param_grid, cv=5)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Print best parameters and final accuracy\n",
        "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "    best_model = grid_search.best_estimator_\n",
        "    y_pred = best_model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Accuracy with best parameters: {accuracy:.3f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pU-blwYX0kb3",
        "outputId": "237b8d7a-034c-4785-d590-2a388645a901"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'max_depth': 10, 'n_estimators': 200}\n",
            "Accuracy with best parameters: 0.965\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 9: Write a Python program to:\n",
        "\n",
        "●\tTrain a Bagging Regressor and a Random Forest Regressor on the California Housing dataset\n",
        "\n",
        "Compare their Mean Squared Errors (MSE)\n",
        "\n",
        " (Include your Python code and output in the code box below.)\n"
      ],
      "metadata": {
        "id": "nh56cvYs0wzL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the California Housing dataset\n",
        "cal_housing = fetch_california_housing()\n",
        "X = cal_housing.data\n",
        "y = cal_housing.target\n",
        "\n",
        "# Split data into train/test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the dataset\n",
        "sc = StandardScaler()\n",
        "X_train_std = sc.fit_transform(X_train)\n",
        "X_test_std = sc.transform(X_test)\n",
        "\n",
        "# Train a Bagging Regressor\n",
        "bagging_model = BaggingRegressor(n_estimators=100, random_state=42)\n",
        "bagging_model.fit(X_train_std, y_train)\n",
        "y_pred_bagging = bagging_model.predict(X_test_std)\n",
        "mse_bagging = mean_squared_error(y_test, y_pred_bagging)\n",
        "print(f\"Bagging Regressor MSE: {mse_bagging:.3f}\")\n",
        "\n",
        "# Train a Random Forest Regressor\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train_std, y_train)\n",
        "y_pred_rf = rf_model.predict(X_test_std)\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "print(f\"Random Forest Regressor MSE: {mse_rf:.3f}\")\n",
        "\n",
        "# Compare MSE\n",
        "print(f\"MSE Difference (Bagging - RF): {mse_bagging - mse_rf:.3f}\")\n",
        "if mse_bagging < mse_rf:\n",
        " print(\"Bagging Regressor performs better.\")\n",
        "elif mse_bagging > mse_rf:\n",
        " print(\"Random Forest Regressor performs better.\")\n",
        "else:\n",
        " print(\"Both models perform equally well.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6C2qpan05EY",
        "outputId": "bd9695ec-2ae1-4ca8-c7fe-abc438207bac"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Regressor MSE: 0.256\n",
            "Random Forest Regressor MSE: 0.255\n",
            "MSE Difference (Bagging - RF): 0.001\n",
            "Random Forest Regressor performs better.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 10: You are working as a data scientist at a financial institution to predict loan default. You have access to customer demographic and transaction history data.You decide to use ensemble techniques to increase model performance. Explain your step-by-step approach to:\n",
        "\n",
        "●\tChoose between Bagging or Boosting\n",
        "\n",
        "●\tHandle overfitting\n",
        "\n",
        "●\tSelect base models\n",
        "\n",
        "●\tEvaluate performance using cross-validation\n",
        "\n",
        "●\tJustify how ensemble learning improves decision-making in this real-world context.\n",
        "\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n"
      ],
      "metadata": {
        "id": "5ZGo9bKE1WwN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: Step-by-Step Approach to Predicting Loan Default Using Ensemble Techniques\n",
        "Step 1: Choose Between Bagging or Boosting\n",
        "- Bagging: Effective for reducing variance and handling unstable models. Suitable if the base models are prone to overfitting.\n",
        "- Boosting: Effective for reducing bias and handling complex relationships. Suitable if the goal is to improve accuracy by focusing on difficult-to-predict instances.\n",
        "\n",
        "Given the complexity of loan default prediction and the need for high accuracy, Boosting might be more suitable.\n",
        "\n",
        "Step 2: Handle Overfitting\n",
        "- Regularization: Use techniques like L1 or L2 regularization in base models to prevent overfitting.\n",
        "- Early Stopping: Implement early stopping in boosting algorithms to stop training when performance on a validation set starts to degrade.\n",
        "- Cross-Validation: Use cross-validation to evaluate model performance and prevent overfitting by ensuring the model generalizes well to unseen data.\n",
        "\n",
        "Step 3: Select Base Models\n",
        "- Decision Trees: Often used as base models in both bagging and boosting due to their simplicity and ability to capture complex interactions.\n",
        "- Other Models: Depending on the dataset, other models like logistic regression or support vector machines could be used as base models.\n",
        "\n",
        "For this example, we'll use Decision Trees as base models.\n",
        "\n",
        "Step 4: Evaluate Performance Using Cross-Validation\n",
        "- Cross-Validation: Split the data into folds and evaluate the model's performance on each fold to get a robust estimate of its generalization ability.\n",
        "\n",
        "Step 5: Justify How Ensemble Learning Improves Decision-Making\n",
        "- Improved Accuracy: Ensemble methods combine multiple models, leading to more accurate predictions and better decision-making.\n",
        "- Robustness: Ensemble methods reduce the impact of individual model errors, making the overall prediction more robust.\n",
        "- Handling Complex Data: Ensemble methods can handle complex datasets with multiple features and interactions, improving the model's ability to capture relevant patterns.\n"
      ],
      "metadata": {
        "id": "UbtF8EAr1i3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import numpy as np\n",
        "\n",
        "# Generate a synthetic dataset for loan default prediction\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=3, random_state=42)\n",
        "\n",
        "# Split data into train/test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Random Forest Classifier (Bagging)\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "print(\"Random Forest Classifier Performance:\")\n",
        "print(classification_report(y_test, y_pred_rf))\n",
        "\n",
        "# Train a Gradient Boosting Classifier (Boosting)\n",
        "gb_model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
        "gb_model.fit(X_train, y_train)\n",
        "y_pred_gb = gb_model.predict(X_test)\n",
        "print(\"Gradient Boosting Classifier Performance:\")\n",
        "print(classification_report(y_test, y_pred_gb))\n",
        "\n",
        "# Evaluate performance using cross-validation\n",
        "rf_scores = cross_val_score(rf_model, X, y, cv=5)\n",
        "gb_scores = cross_val_score(gb_model, X, y, cv=5)\n",
        "print(f\"Random Forest CV Accuracy: {np.mean(rf_scores):.3f} (+/- {np.std(rf_scores):.3f})\")\n",
        "print(f\"Gradient Boosting CV Accuracy: {np.mean(gb_scores):.3f} (+/- {np.std(gb_scores):.3f})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gq3hNk9U2CfC",
        "outputId": "19302a87-82a0-4c9a-84c2-a814f6a3bdcc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Classifier Performance:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.98      0.94        94\n",
            "           1       0.98      0.92      0.95       106\n",
            "\n",
            "    accuracy                           0.94       200\n",
            "   macro avg       0.95      0.95      0.94       200\n",
            "weighted avg       0.95      0.94      0.95       200\n",
            "\n",
            "Gradient Boosting Classifier Performance:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.93      0.92        94\n",
            "           1       0.93      0.92      0.93       106\n",
            "\n",
            "    accuracy                           0.93       200\n",
            "   macro avg       0.92      0.93      0.92       200\n",
            "weighted avg       0.93      0.93      0.93       200\n",
            "\n",
            "Random Forest CV Accuracy: 0.911 (+/- 0.019)\n",
            "Gradient Boosting CV Accuracy: 0.887 (+/- 0.017)\n"
          ]
        }
      ]
    }
  ]
}